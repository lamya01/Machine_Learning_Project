# -*- coding: utf-8 -*-
"""ml project 12

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WG2L1dBwIeRzuWsJrBbPArzK0K9E33iM
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from tqdm import tqdm_notebook, trange
from tqdm import tqdm
import sys
import time
from sklearn.metrics import accuracy_score, classification_report
from sklearn import decomposition
from sklearn import datasets
from mpl_toolkits.mplot3d import Axes3D

class NeuralNet():
  
  def __init__(self, architecture):
    self.architecture = architecture
    self.params = self._initialize_params(architecture)
  
  def _initialize_params(self, architecture):
    params = {}
    for id_, layer in enumerate(architecture):
      layer_id = id_ + 1

      input_dim = layer['input_dim']
      output_dim = layer['output_dim']

      params['W'+str(layer_id)] = np.random.randn(output_dim, input_dim)*0.1
      params['b'+str(layer_id)] = np.zeros((output_dim, 1))

    return params
  
  def sigmoid(self, Z):
    return 1/(1+np.exp(-Z))
  def relu(self, Z):
    return np.maximum(0, Z)

  def sigmoid_backward(self, dA, z_curr):
    sig = self.sigmoid(z_curr)
    return sig*(1-sig)*dA

  def relu_backward(self, dA, z_curr):
    dz = np.array(dA, copy=True)
    dz[z_curr<=0]=0
    return dz
  
  def _forward_prop_this_layer(self, A_prev, W_curr, b_curr, activation_function):
    z_curr = np.dot(W_curr, A_prev) + b_curr

    if activation_function is 'relu':
      activation = self.relu
    elif activation_function is 'sigmoid':
      activation = self.sigmoid
    else:
      raise Exception(f"{activation_function} is not supported, Only sigmoid, relu are supported")

    return activation(z_curr), z_curr

  def _forward(self, X):
    cache = {}
    A_current = X
    for layer_id_prev, layer in enumerate(self.architecture):
      current_layer_id = layer_id_prev+1

      A_previous = A_current
      activation = layer['activation']

      W_curr = self.params['W'+str(current_layer_id)]
      b_curr = self.params['b'+str(current_layer_id)]

      A_current, Z_curr = self._forward_prop_this_layer(A_previous, W_curr,
                                                  b_curr, activation)

      cache['A'+str(layer_id_prev)] = A_previous
      cache['Z'+str(current_layer_id)] = Z_curr
      
    return A_current, cache

  def _criterion(self, y, yhat):
    m = yhat.shape[1]
    cost = -1/m * (np.dot(y, np.log(yhat).T) + np.dot(1-y, np.log(1-yhat).T))
    return np.squeeze(cost)
  
  def _backprop_this_layer(self, da_curr, z_curr, W_curr, b_curr, A_prev, activation_function):
    if activation_function is 'sigmoid':
      activation_back = self.sigmoid_backward
    elif activation_function is 'relu':
      activation_back = self.relu_backward
    else:
      raise Exception('need sigmoid or relu')
    m = A_prev.shape[1]

    dz_curr = activation_back(da_curr, z_curr)
    dw_curr = np.dot(dz_curr, A_prev.T)/m
    db_curr = np.sum(dz_curr, axis=1, keepdims=True)/m
    da_prev = np.dot(W_curr.T, dz_curr)

    return da_prev, dw_curr, db_curr
  
  def _backward(self, ytrue, ypred, cache):
    grads = {}
    m = ytrue.shape[1]
    da_prev = np.divide(1-ytrue, 1-ypred) - np.divide(ytrue, ypred)
    
    for prev_layer_id, layer in reversed(list(enumerate(self.architecture))):
      layer_id = prev_layer_id + 1
      activation = layer['activation']

      da_curr = da_prev

      A_prev = cache['A'+str(prev_layer_id)]
      Z_curr = cache['Z'+str(layer_id)]

      W_curr = self.params['W'+str(layer_id)]
      b_curr = self.params['b'+str(layer_id)]

      da_prev, dw_curr, db_curr = self._backprop_this_layer(
          da_curr, Z_curr, W_curr, b_curr, A_prev, activation)

      grads["dw"+str(layer_id)] = dw_curr
      grads['db'+str(layer_id)] = db_curr

    return grads
  
  def update(self, grads, learning_rate):
    for layer_id, layer in enumerate(self.architecture, 1):
      self.params['W'+str(layer_id)] -= learning_rate * grads['dw'+str(layer_id)]
      self.params['b'+str(layer_id)] -= learning_rate * grads['db'+str(layer_id)]
  
  def fit(self, X, y, epochs, learning_rate, verbose=True, show_loss=True):
    X, y = X.T, y.reshape((y.shape[0],-1)).T
    loss_history, accuracy_history = [], []
    for epoch in tqdm_notebook(range(epochs), total=epochs, unit='epoch'):
      yhat, cache = self._forward(X)
      loss = self._criterion(y, yhat)
      loss_history.append(loss)

      yacc = yhat.copy()
      yacc[yacc>0.5] = 1
      yacc[yacc<=0.5] = 0

      accuracy = np.sum(y[0]==yacc[0])/(yacc.shape[1])
      accuracy_history.append(accuracy)

      grads_values = self._backward(y, yhat, cache)

      self.update(grads_values, learning_rate)
      if(epoch % 1000 == 0):
              if(verbose):
                  print("Epoch: {:05} - cost: {:.5f} - accuracy: {:.5f}".format(epoch, loss, accuracy))

        
    fig = plt.figure(figsize=(12,10))
    plt.plot(range(epochs), loss_history, 'r-')
    plt.plot(range(epochs), accuracy_history, 'b--')
    plt.legend(['Training_loss', 'Training_Accuracy'])
    plt.xlabel('Epochs')
    plt.ylabel('Loss/Accuracy')
    plt.show()

  def predict(self, X):
    yhat, _ = self._forward(X)
    yhat[yhat>0.6]=1
    yhat[yhat<=0.6]=0
    return np.squeeze(yhat)



from google.colab import drive
drive.mount('/content/drive')

from pandas import read_csv #from here till the markerrrrrr
df=read_csv('/content/drive/My Drive/ml data/cat3.csv')



#Dataset
X_pre=df[["i","z","extinction_u","extinction_g","nuv-u","nuv-g","nuv-r","u-i","u-z","g-r","r-i","r-z","nuv-z","u-g","u-r","nuv_mag"]]
#X_pre1=X_pre.values
#X=X_pre1
#X=X_pre1[:15000,:]
Y_pre=df[["class"]]

#Y_pre1=Y_pre.values
#y=Y_pre1
#y=Y_pre1[:15000,:]
#print(X.shape)
#in_dim=X.shape[1]
X = X_pre.loc[:, X_pre.columns != 'class']
y = Y_pre.loc[:, Y_pre.columns == 'class']
#X=X.values
#y=y.values
print(X.shape)
in_dim=X.shape[1]

print(in_dim)

from imblearn.over_sampling import SMOTE
os = SMOTE(random_state=0)
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.1, random_state=0)
columns = X_train1.columns
os_data_X,os_data_y=os.fit_sample(X_train1, y_train1)
os_data_X = pd.DataFrame(data=os_data_X,columns=columns )
os_data_y= pd.DataFrame(data=os_data_y,columns=['class'])
# we can Check the numbers of our data
print("length of oversampled data is ",len(os_data_X))
print("Number of no subscription in oversampled data",len(os_data_y[os_data_y['class']==0]))
print("Number of subscription",len(os_data_y[os_data_y['class']==1]))
print("Proportion of no subscription data in oversampled data is ",len(os_data_y[os_data_y['class']==0])/len(os_data_X))
print("Proportion of subscription data in oversampled data is ",len(os_data_y[os_data_y['class']==1])/len(os_data_X))



os_data_X.shape
os_data_y.shape

X_train, X_test, y_train, y_test = train_test_split(os_data_X, os_data_y, stratify=os_data_y, test_size=0.3, random_state=42)

#till here run as it is inclusive
X_train=X_train.values
y_train=y_train.values
X_test=X_test.values
y_test=y_test.values

NN_ARCHITECTURE = [
    {"input_dim": in_dim, "output_dim": 16, "activation": "relu"}, # Input Layer
    {"input_dim": 16, "output_dim": 32, "activation": "relu"},# Hidden Layer -- 1 Second Hidden Layer
    {"input_dim": 32, "output_dim": 16, "activation": "relu"},#{"input_dim": 16, "output_dim": 8, "activation": "relu"},# Third Hidden Layer,
    {"input_dim": 16, "output_dim": 1, "activation": "sigmoid"}# Output Layer
]

net = NeuralNet(NN_ARCHITECTURE)
net.fit(X_train, y_train, epochs=80001, learning_rate=0.005, verbose=True, show_loss=True)





import pickle
with open('config.dictionary1', 'wb') as model_dictionary_file:
 
  # Step 3
  pickle.dump(net, model_dictionary_file)

with open('config.dictionary1', 'rb') as config_dictionary_file:
 
    # Step 3
    net2 = pickle.load(config_dictionary_file)
 
    # After config_dictionary is read from file
    #print(config_dictionary)

net2

predictions = net.predict(X_test.T)
accuracy_score(y_test, predictions)

print(classification_report(y_test, predictions))

df1=read_csv('/content/drive/My Drive/ml data/cat4.csv')
#"fuv-nuv","fuv-u","fuv-g","fuv-r"
X_test1=df1[["i","z","extinction_u","extinction_g","nuv-u","nuv-g","nuv-r","u-i","u-z","g-r","r-i","r-z","nuv-z","u-g","u-r","nuv_mag"]]

X_test2=X_test1.values
X_test2.shape

Y_test1=df1[["class"]]
Y_test2=Y_test1.values

X_train2, X_tes2, y_train2, y_tes2 = train_test_split(X_test2, Y_test2, stratify=Y_test2, test_size=0.99, random_state=42)

print(X_tes2.shape)
predictions = net.predict(X_tes2.T)
accuracy_score(y_tes2, predictions)

print(classification_report(y_tes2, predictions))

#PCA
from google.colab import drive
drive.mount('/content/drive')

from pandas import read_csv
cat1_data=read_csv('/content/drive/My Drive/ml data/cat4.csv')

cat1_data.head(10)

y=cat1_data[["class"]]
y=y.values
print(len(cat1_data),len(y))
cat1_data=cat1_data.drop(['class'], axis = 1)
cat1_data=cat1_data.drop(['pred'],axis=1)
cat1_data=cat1_data.drop(['spectrometric_redshift'],axis=1)

cat1_data.describe()

import matplotlib.pyplot as plt

import seaborn as sns



correlation_matrix=cat1_data.corr()

mask = np.zeros_like(correlation_matrix, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(correlation_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

plt.scatter(cat1_data["nuv-z"],cat1_data["fuv-z"])

pca = decomposition.PCA(n_components=8)
pca.fit(cat1_data)
PCA_transformed_data = pca.transform(cat1_data)

cat1_data.head(10)

cat1_data.index

print(cat1_data.loc[0])

PCA_transformed_data[0]

PCA_Data=pd.DataFrame(PCA_transformed_data)

PCA_Data.describe()

pca_correlation_matrix=PCA_Data.corr()

mask = np.zeros_like(pca_correlation_matrix, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(pca_correlation_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

print(pca_correlation_matrix)

PCA_Data

X_pre=PCA_Data[[0,1,2,3,4,5,6,7]]
X_pre1=X_pre.values
X=X_pre1
Y_pre1=y

#Y_pre1=Y_pre.values
#y=Y_pre1[:15000,:]
print(X.shape)
in_dim=X.shape[1]
#y=y.to_numpy()
#y.reshape((649,1))
print(y.shape)
su=0
for i in range(649):
    if y[i]==1:
      su=su+1
print(su/649)
print(in_dim)

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
#df.dtypes
PCA_Data.dtypes

type(X_train)

#in_dim=8
NN_ARCHITECTURE = [
    {"input_dim": in_dim, "output_dim": 8, "activation": "relu"}, # Input Layer
    {"input_dim": 8, "output_dim": 32, "activation": "relu"},# Hidden Layer -- 1
    {"input_dim": 32, "output_dim": 64, "activation": "relu"},# Second Hidden Layer
    #{"input_dim": 16, "output_dim": 8, "activation": "relu"},# Third Hidden Layer
    {"input_dim": 64, "output_dim": 1, "activation": "sigmoid"},# Output Layer
]

#import numpy as np
net = NeuralNet(NN_ARCHITECTURE)
#x_train=PCA_transformed_data
#y_train=y.to_numpy()
net.fit(X_train,y_train, epochs=100001, learning_rate=0.03, verbose=True, show_loss=True)

len(x_train[0])

#X_pre printed HERE!
print(X_pre1.shape)

"""CHECKING FORMATS FROM HERE"""

df.shape

(df.loc[0]).shape

df.describe()

df.head(10)

y=df[["class"]]

y.shape

df=df.drop(['class'], axis = 1)
df=df.drop(['spectrometric_redshift'],axis=1)
df=df.drop(['pred'],axis=1)

#checking if whether whatever i dropped is actually dropped
df.head(10)

df.iloc[0]

pca = decomposition.PCA(n_components=10)
pca.fit(df)
PCA_transformed_data = pca.transform(df)

PCA_transformed_data[0]

df.iloc[0]

PCA_DATA=pd.DataFrame(PCA_transformed_data)

PCA_DATA.iloc[0]

#print(df[0])
print(PCA_DATA[0])

df[0]

print(PCA_DATA)

print(df)

print(type(df.iloc[0]))
print(type(PCA_DATA[0]))

print(len(df),len(PCA_DATA))

X_pre=PCA_DATA

print(X_pre)

#X_pre=PCA_Data
X_pre1=X_pre.values

print(X_pre.values)

X=X_pre1
Y_pre1=y

print(X)
print(Y_pre1)

Y_pre1=Y_pre.values

print(Y_pre1)

y=Y_pre1[:33463,:]
print(X.shape)

in_dim=X.shape[1]
print(in_dim)

print(y.shape)

su=0
for i in range(33463):
    if y[i]==1:
      su=su+1
print(su/649)
print(in_dim)

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#in_dim=8
NN_ARCHITECTURE = [
    {"input_dim": in_dim, "output_dim": 10, "activation": "relu"}, # Input Layer
    {"input_dim": 10, "output_dim": 32, "activation": "relu"},# Hidden Layer -- 1
    {"input_dim": 32, "output_dim": 64, "activation": "relu"},# Second Hidden Layer
    #{"input_dim": 16, "output_dim": 8, "activation": "relu"},# Third Hidden Layer
    {"input_dim": 64, "output_dim": 1, "activation": "sigmoid"},# Output Layer
]

net = NeuralNet(NN_ARCHITECTURE)
net.fit(X_train,y_train, epochs=100001, learning_rate=0.03, verbose=True, show_loss=True)

